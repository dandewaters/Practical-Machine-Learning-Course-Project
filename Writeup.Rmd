---
title: "Practical Machine Learning Course Project"
author: "Daniel DeWaters"
date: "12/2/2019"
output:
  html_document:
    toc: true
    top_depth: 2
    toc_float: true
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(caret)
library(corrplot)

```

# Synopsis

The purpose of this assignment is to create a model that predicts when a person is performing an exercise correctly or incorrectly. This model will be trained from the weight lifting data set:

http://groupware.les.inf.puc-rio.br/har


# Getting the Data
```{r getData}
sample_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
quiz_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

sample_file_name <- "./data/pml-training.csv"
quiz_file_name <- "./data/pml-testing.csv"

# Download files
if(!file.exists(sample_file_name) | !file.exists(quiz_file_name)){
  download.file(sample_url, destfile=training_file_name, method="curl")
  download.file(quiz_url, destfile=testing_file_name, method="curl")
}

# Read files
sample_data <- read.csv(sample_file_name, na.string=c("NA", "#DIV/0!"))
quiz_data <- read.csv(quiz_file_name, na.string=c("NA", "#DIV/0!"))
```

# Cleaning the Data

Our training set is `r dim(sample_data)[2]` columns by `r dim(sample_data)[1]` rows. Let's see if we can reduce the size a bit to make things easier on ourselves. First we should remove columns from the data set that are irrelevant to training a model (names, timestanps, IDs) and then remove columns that are mostly NAs or have near zero variance. The training dataset has a total of `r sum(is.na(sample_data))` missing values.

```{r cleanData}
# Remove columns that are irrelevant to prediction (names, timestamps, row IDs)
sample_data <- sample_data[,-c(1:7)]
quiz_data <- quiz_data[,-c(1:7)]
# Remove columns that have greater than 5% missing values 
mean_not_nas <- function(x){mean(!is.na(x)) > 0.95}
good_mean_nas <- sapply(sample_data, mean_not_nas)
sample_data <- sample_data[, good_mean_nas]
quiz_data <- quiz_data[, good_mean_nas]

sum_nas <- sum(!is.na(sample_data))
```

Our sample set is now `r dim(sample_data)[2]` columns by `r dim(sample_data)[1]` rows. It has `r sum_nas` missing values. Now we can break up the sample set into a training and testing set.

# Creating A Training and Testing Set - (Cross Validation)

In order to test our model's accuracy before we take the quiz, we have to make a training and test set.

```{r createDataPart}
set.seed(4321)

# choose which indeces will be put in training set
inTrain <- createDataPartition(y=sample_data$classe, p=0.7, list=FALSE)

# Separate sample set into training and testing data frames
training <- sample_data[inTrain,]
testing <- sample_data[-inTrain,]
```

# Building Models

I chose to build a random forest and a general boosting model because they are some of the most popular and accurate prediction models for a classification problem.

## Random Forest
```{r build_RF_model, cache=TRUE}
rf_cont <- trainControl(method="cv", number=3, verboseIter=FALSE)
rf_fit <- train(classe~., data=training, method="rf", trControl=rf_cont, verbose=FALSE)
rf_fit$finalModel
```

### In Sample Error

Let's check the accuracy of the model using the data that it was trained with.

```{r rf_inSampleError}
rf_train_pred <- predict(rf_fit, training)
rf_train_accuracy <- confusionMatrix(training$classe, rf_train_pred)$overall[1]
rf_train_accuracy
```

Our accuracy with the training data is `r round(100*rf_train_accuracy, digits=2)`%. The random forest model has no in sample error.

### Out of Sample Error

Now we can check the accuracy of the model using new data. 

```{r rf_outOfSampleError}
rf_test_pred <- predict(rf_fit, testing)
rf_test_accuracy <- confusionMatrix(testing$classe, rf_test_pred)$overall[1]
rf_test_accuracy
```

With the testing data, our model accuracy is `r round(100*rf_test_accuracy, digits=2)`%, so our model has `r 100-round(100*rf_test_accuracy, digits=2)`% out of sample error.

## Boosting
```{r buildBoostingModel, cache=TRUE}
gb_cont <- trainControl(method="repeatedcv", number=3, verboseIter=FALSE)
gb_fit <- train(classe~., data=training, method="gbm", trControl=gb_cont, verbose=FALSE)
gb_fit$finalModel
```

### In Sample Error

Let's check the accuracy of the model using the data that it was trained with.

```{r gb_inSampleError}
gb_train_pred <- predict(gb_fit, training)
gb_train_accuracy <- confusionMatrix(training$classe, gb_train_pred)$overall[1]
gb_train_accuracy
```

Our accuracy with the training data is `r round(100*gb_train_accuracy, digits=2)`%, so this model has `r 100-round(100*gb_train_accuracy, digits=2)`%

### Out of Sample Error

Now we can check the accuracy of the model using new data. 

```{r gb_outOfSampleError}
gb_test_pred <- predict(gb_fit, testing)
gb_test_accuracy <- confusionMatrix(testing$classe, gb_test_pred)$overall[1]
gb_test_accuracy
```

With the testing data, this model accuracy is `r round(100*gb_test_accuracy, digits=2)`%, so our model has `r 100-round(100*gb_test_accuracy, digits=2)`% out of sample error.

# Comparison of Model Accuracy

|      Model     |    In Sample Error    | Out of Sample Error  |
|----------------|-----------------------|----------------------|
| Random Forest  |`r 1-rf_train_accuracy`|`r 1-rf_test_accuracy`|
|    Boosting    |`r 1-gb_train_accuracy`|`r 1-gb_test_accuracy`|

I chose to use the random forest model to predict the quiz answers because it has a smaller out of sample error.

```{r quiz}
# Find quiz answers
quiz_answers <- predict(rf_fit, quiz_data)
quiz_answers
```

# References

Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.